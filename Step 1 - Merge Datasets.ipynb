{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93fa3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: PyPDF2 in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joeynewfield/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2072af8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading PDF file...\n",
      "\n",
      "PDF read successfully. First 500 characters:\n",
      "DATE 05/06/24                                  FLORIDA DEPARTMENT OF TRANSPORTATION\n",
      "                                                           TRAFFIC COUNTS\n",
      "                                                HOURLY CONTINUOUS COUNTS FINAL REPORT\n",
      "                                                           JANUARY 2023\n",
      " \n",
      " COUNTY NAME:  OKALOOSA      STATION:  0386    DIRECTION:  E  LANE:  0\n",
      " DESCRIPTION: SR 30/US-98, 300'  W OF CR-30C /BCH DR                 \n",
      " LOCATION: COUNTY 57 SECTION 030 SUBSECTI\n",
      "...\n",
      "\n",
      "Parsing traffic data...\n",
      "Total lines in PDF: 1186\n",
      "Found month: JANUARY\n",
      "Found direction: E\n",
      "Found month: JANUARY\n",
      "Found direction: W\n",
      "Found month: FEBRUARY\n",
      "Found direction: E\n",
      "Found month: FEBRUARY\n",
      "Found direction: W\n",
      "Found month: MARCH\n",
      "Found direction: E\n",
      "Found month: MARCH\n",
      "Found direction: W\n",
      "Found month: APRIL\n",
      "Found direction: E\n",
      "Found month: APRIL\n",
      "Found direction: W\n",
      "Found month: MAY\n",
      "Found direction: E\n",
      "Found month: MAY\n",
      "Found direction: W\n",
      "Found month: JUNE\n",
      "Found direction: E\n",
      "Found month: JUNE\n",
      "Found direction: W\n",
      "Found month: JULY\n",
      "Found direction: E\n",
      "Found month: JULY\n",
      "Found direction: W\n",
      "Found month: AUGUST\n",
      "Found direction: E\n",
      "Found month: AUGUST\n",
      "Found direction: W\n",
      "Found month: SEPTEMBER\n",
      "Found direction: E\n",
      "Found month: SEPTEMBER\n",
      "Found direction: W\n",
      "Found month: OCTOBER\n",
      "Found direction: E\n",
      "Found month: OCTOBER\n",
      "Found direction: W\n",
      "Found month: NOVEMBER\n",
      "Found direction: E\n",
      "Found month: NOVEMBER\n",
      "Found direction: W\n",
      "Found month: DECEMBER\n",
      "Found direction: E\n",
      "Found month: DECEMBER\n",
      "Found direction: W\n",
      "\n",
      "Parsed data summary:\n",
      "Total records: 15672\n",
      "Date range: 2023-01-01 to 2023-12-31\n",
      "Unique directions: ['E' 'W']\n",
      "\n",
      "Processing weather data...\n",
      "\n",
      "Weather data summary:\n",
      "Total records: 9042\n",
      "Date range: 2023-01-01 to 2023-12-31\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "Merged data summary:\n",
      "Total records: 16107\n",
      "Date range: 2023-01-01 to 2023-12-31\n",
      "\n",
      "Data successfully processed and saved to 'merged_traffic_weather_data.csv'\n",
      "\n",
      "Final Dataset Summary:\n",
      "Total records: 16107\n",
      "Date range: 2023-01-01 to 2023-12-31\n",
      "Unique directions: ['E' 'W']\n",
      "\n",
      "Average traffic by direction:\n",
      "direction\n",
      "E    909.919826\n",
      "W    890.064153\n",
      "Name: traffic_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "def read_pdf_content(pdf_path):\n",
    "    \"\"\"Read content from PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text_content = \"\"\n",
    "        for page in reader.pages:\n",
    "            text_content += page.extract_text() + \"\\n\"\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def parse_traffic_pdf(text_content):\n",
    "    \"\"\"Parse traffic data from the Destin Traffic Data format.\"\"\"\n",
    "    if not text_content:\n",
    "        print(\"No text content to parse\")\n",
    "        return None\n",
    "        \n",
    "    traffic_data = []\n",
    "    current_month = None\n",
    "    current_year = \"2023\"\n",
    "    direction = None\n",
    "    \n",
    "    # Split into lines and remove empty lines\n",
    "    lines = [line.strip() for line in text_content.split('\\n') if line.strip()]\n",
    "    \n",
    "    print(f\"Total lines in PDF: {len(lines)}\")\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        # Extract month\n",
    "        if 'HOURLY CONTINUOUS COUNTS FINAL REPORT' in line:\n",
    "            for next_line in lines[i:i+3]:\n",
    "                month_match = re.search(r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s+2023', next_line)\n",
    "                if month_match:\n",
    "                    current_month = month_match.group(1)\n",
    "                    print(f\"Found month: {current_month}\")\n",
    "                    break\n",
    "        \n",
    "        # Extract direction\n",
    "        if 'DIRECTION:' in line:\n",
    "            direction = line.split('DIRECTION:')[1].split()[0]\n",
    "            print(f\"Found direction: {direction}\")\n",
    "            \n",
    "        # Process data rows\n",
    "        if re.match(r'^\\d{1,2}\\s+[SMTWRFA]', line):\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                day = int(parts[0])\n",
    "                \n",
    "                # Extract the 24 hourly counts\n",
    "                hourly_data = parts[2:26]  # Skip day number and day code\n",
    "                if len(hourly_data) == 24:\n",
    "                    # Process each hour\n",
    "                    for hour, count_str in enumerate(hourly_data):\n",
    "                        # Clean the count string (remove any non-digit characters)\n",
    "                        count = int(''.join(filter(str.isdigit, count_str)))\n",
    "                        \n",
    "                        # Create date string\n",
    "                        if current_month and direction:\n",
    "                            month_num = datetime.strptime(current_month, '%B').month\n",
    "                            date_str = f\"{current_year}-{month_num:02d}-{day:02d}\"\n",
    "                            \n",
    "                            traffic_data.append({\n",
    "                                'date': date_str,\n",
    "                                'hour': hour,\n",
    "                                'traffic_count': count,\n",
    "                                'direction': direction,\n",
    "                                'day_code': parts[1]\n",
    "                            })\n",
    "                \n",
    "            except (ValueError, IndexError) as e:\n",
    "                continue\n",
    "    \n",
    "    if not traffic_data:\n",
    "        print(\"No traffic data was parsed\")\n",
    "        return None\n",
    "        \n",
    "    df = pd.DataFrame(traffic_data)\n",
    "    \n",
    "    print(f\"\\nParsed data summary:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Unique directions: {df['direction'].unique()}\")\n",
    "    \n",
    "    daily_totals = df.groupby(['date', 'direction'])['traffic_count'].sum().reset_index()\n",
    "    daily_totals = daily_totals.rename(columns={'traffic_count': 'daily_total'})\n",
    "    df = pd.merge(df, daily_totals, on=['date', 'direction'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_weather_data(weather_df):\n",
    "    \"\"\"Clean and process weather data.\"\"\"\n",
    "    try:\n",
    "        # Convert timestamp - handle the UTC format properly\n",
    "        weather_df['timestamp'] = pd.to_datetime(\n",
    "            weather_df['dt_iso'].str.replace(' \\+0000 UTC', '', regex=True),\n",
    "            format='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        # Extract relevant columns and rename for clarity\n",
    "        cleaned_df = weather_df[[\n",
    "            'timestamp', 'temp', 'humidity', 'wind_speed', \n",
    "            'visibility', 'rain_1h', 'weather_main'\n",
    "        ]].copy()\n",
    "        \n",
    "        # Rename columns\n",
    "        cleaned_df = cleaned_df.rename(columns={\n",
    "            'temp': 'temperature',\n",
    "            'rain_1h': 'precipitation'\n",
    "        })\n",
    "        \n",
    "        # Handle missing values\n",
    "        cleaned_df['precipitation'] = cleaned_df['precipitation'].fillna(0)\n",
    "        cleaned_df = cleaned_df.fillna(method='ffill', limit=3)\n",
    "        cleaned_df = cleaned_df.dropna()\n",
    "        \n",
    "        # Add date and hour columns for merging\n",
    "        cleaned_df['date'] = cleaned_df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "        cleaned_df['hour'] = cleaned_df['timestamp'].dt.hour\n",
    "        \n",
    "        print(\"\\nWeather data summary:\")\n",
    "        print(f\"Total records: {len(cleaned_df)}\")\n",
    "        print(f\"Date range: {cleaned_df['date'].min()} to {cleaned_df['date'].max()}\")\n",
    "        \n",
    "        return cleaned_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning weather data: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def merge_datasets(traffic_df, weather_df):\n",
    "    \"\"\"Merge traffic and weather datasets.\"\"\"\n",
    "    if traffic_df is None or weather_df is None:\n",
    "        print(\"Cannot merge datasets - missing data\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Merge on date and hour\n",
    "        merged_df = pd.merge(\n",
    "            traffic_df,\n",
    "            weather_df,\n",
    "            on=['date', 'hour'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Add temporal features\n",
    "        merged_df['timestamp'] = pd.to_datetime(merged_df['date']) + pd.to_timedelta(merged_df['hour'], unit='h')\n",
    "        merged_df['day_of_week'] = merged_df['timestamp'].dt.day_name()\n",
    "        merged_df['is_weekend'] = merged_df['timestamp'].dt.dayofweek.isin([5, 6])\n",
    "        merged_df['month'] = merged_df['timestamp'].dt.month\n",
    "        \n",
    "        # Reorder columns\n",
    "        column_order = [\n",
    "            'timestamp', 'date', 'hour', 'day_of_week', 'is_weekend', 'month',\n",
    "            'direction', 'traffic_count', 'daily_total', 'temperature', 'humidity', \n",
    "            'wind_speed', 'visibility', 'precipitation', 'weather_main'\n",
    "        ]\n",
    "        \n",
    "        merged_df = merged_df[column_order]\n",
    "        \n",
    "        print(f\"\\nMerged data summary:\")\n",
    "        print(f\"Total records: {len(merged_df)}\")\n",
    "        print(f\"Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error merging datasets: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Read the PDF file\n",
    "        print(\"Reading PDF file...\")\n",
    "        traffic_text = read_pdf_content('Destin Traffic Data.pdf')\n",
    "        \n",
    "        if traffic_text:\n",
    "            print(\"\\nPDF read successfully. First 500 characters:\")\n",
    "            print(traffic_text[:500])\n",
    "            print(\"...\")\n",
    "        \n",
    "        # Parse traffic data\n",
    "        print(\"\\nParsing traffic data...\")\n",
    "        traffic_df = parse_traffic_pdf(traffic_text)\n",
    "        \n",
    "        if traffic_df is not None:\n",
    "            # Read and clean weather data\n",
    "            print(\"\\nProcessing weather data...\")\n",
    "            weather_df = pd.read_csv('Destin Weather Data.csv')\n",
    "            weather_df = clean_weather_data(weather_df)\n",
    "            \n",
    "            if weather_df is not None:\n",
    "                # Merge datasets\n",
    "                print(\"\\nMerging datasets...\")\n",
    "                combined_df = merge_datasets(traffic_df, weather_df)\n",
    "                \n",
    "                if combined_df is not None:\n",
    "                    # Save to CSV\n",
    "                    output_file = 'merged_traffic_weather_data.csv'\n",
    "                    combined_df.to_csv(output_file, index=False)\n",
    "                    print(f\"\\nData successfully processed and saved to '{output_file}'\")\n",
    "                    \n",
    "                    # Display summary statistics\n",
    "                    print(\"\\nFinal Dataset Summary:\")\n",
    "                    print(f\"Total records: {len(combined_df)}\")\n",
    "                    print(f\"Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "                    print(f\"Unique directions: {combined_df['direction'].unique()}\")\n",
    "                    print(\"\\nAverage traffic by direction:\")\n",
    "                    print(combined_df.groupby('direction')['traffic_count'].mean())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in processing: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078ee4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
